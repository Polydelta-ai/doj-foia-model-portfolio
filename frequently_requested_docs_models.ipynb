{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START BY CREATING A CONDA ENV USING ENVIRONMENT.YML THEN USE THAT ENV AS THE KERNEL FOR THIS NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import mlflow\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from frequently_requested_docs.docs_helper import getModel, getSaveName, loadEmbeddings, getEmbeddingPath\n",
    "from frequently_requested_docs.docs_config import TOP_K, MODEL_NAMES, DATA_CSV_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "princeton-nlp-unsup-simcse-roberta-large princeton-nlp/unsup-simcse-roberta-large\n",
      "Loading model from disc\n",
      "<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n"
     ]
    }
   ],
   "source": [
    "# List of models optimized for semantic textual similarity can be found at:\n",
    "# https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0\n",
    "\n",
    "# MODEL_NAMES = [\n",
    "#     'nli-mpnet-base-v2',\n",
    "#     'nli-roberta-base-v2',\n",
    "#     'princeton-nlp/sup-simcse-roberta-large',\n",
    "#     'princeton-nlp/unsup-simcse-roberta-large',\n",
    "#     'stsb-distilroberta-base-v2',\n",
    "#     'stsb-mpnet-base-v2',\n",
    "#     'stsb-roberta-base',\n",
    "#     'stsb-roberta-base-v2',\n",
    "#     'stsb-roberta-large',\n",
    "# ]\n",
    "\n",
    "# look at MODEL_NAMES in config.py for more model names to test\n",
    "model_name = 'princeton-nlp/unsup-simcse-roberta-large'\n",
    "save_name = getSaveName(model_name)\n",
    "\n",
    "print(save_name, model_name)\n",
    "    \n",
    "model = getModel(model_name, save_name)\n",
    "print(type(model))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Top K most similar docs from freqdoc dataset given a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component                                                 FEMA\n",
      "Document                             FEMA Clothing- XYZ Legend\n",
      "Tag          FEMA Individual and Public Assistance Claims P...\n",
      "URL          https://www.dhs.gov/sites/default/files/public...\n",
      "Agency                                                     DHS\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Format of corpus sentences\n",
    "corpus_docs = []\n",
    "data = pd.read_csv(DATA_CSV_PATH)\n",
    "data.reset_index()\n",
    "\n",
    "for ind, row in data.iterrows():\n",
    "    if isinstance(row['Document'], str):\n",
    "        corpus_docs.append(row)\n",
    "        \n",
    "print(corpus_docs[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed embeddings from disc\n"
     ]
    }
   ],
   "source": [
    "# Load corpus embeddings if exist, otherwise encode embeddings\n",
    "embedding_path = getEmbeddingPath(save_name)\n",
    "corpus_embeddings = None\n",
    "            \n",
    "corpus_docs, corpus_embeddings = loadEmbeddings(model, embedding_path, corpus_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 or more sentences\n",
    "examples = ['I am searching for the Detention Facility Reviews for the Randall County Jail in Amarillo, Texas', 'Statements made by former georgia senator david perdue about visas.', 'All documents regarding the TSA’s throughput data for August 2017']\n",
    "sentence = examples[2]\n",
    "\n",
    "# encode sentence to get sentence embeddings\n",
    "sentence_embedding = model.encode(sentence, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: All documents regarding the TSA’s throughput data for August 2017 \n",
      "\n",
      "Top 25 most similar sentences in corpus:\n",
      "August 2017 FOIA Log (Score: 0.7429)\n",
      "FY18 August FOIA Logs  (Score: 0.7229)\n",
      "TSA Throughput Data July 9, 2017 to July 15, 2017 (Score: 0.6852)\n",
      "August 2016 FOIA Log (Score: 0.6851)\n",
      "TSA Throughput Data July 30, 2017 to August 5, 2017 (Score: 0.6828)\n",
      "August 2018 FOIA Log (Score: 0.6811)\n",
      "August 2015 FOIA Log (Score: 0.6795)\n",
      "TSA Throughput Data July 2, 2017 to July 8, 2017 (Score: 0.6714)\n",
      "August 2010 FOIA Log (Score: 0.6644)\n",
      "TSA Throughput Data July 16, 2017 to July 22, 2017 (Score: 0.6630)\n",
      "FOIA Log August 2017 (Score: 0.6606)\n",
      "TSA Throughput Data July 8, 2018 to July 14, 2018 Page Count: 942 (Score: 0.6575)\n",
      "TSA Throughput Data August 13, 2017 to August 19, 2017 (Score: 0.6548)\n",
      "July 2016 FOIA Log (Score: 0.6528)\n",
      "TSA Throughput Data July 15, 2018 to July 21, 2018 (Score: 0.6523)\n",
      "TSA Throughput Data August 20, 2017 to August 26, 2017 (Score: 0.6491)\n",
      "FOIA Log August 2016 (Score: 0.6460)\n",
      "July 2017 FOIA Log (Score: 0.6459)\n",
      "DHS OBIM Office FOIA Log - FY 2020 (July - September) (Score: 0.6407)\n",
      "FOIA Monthly Logs August 2015 (Score: 0.6353)\n",
      "TSA Throughput Data August 19, 2018 to August 25, 2018 (Score: 0.6330)\n",
      "TSA Throughput Data August 2, 2020  to August 8, 2020 (Score: 0.6329)\n",
      "July 2015 FOIA Log (Score: 0.6301)\n",
      "TSA Throughput Data August 6, 2017 to August 12, 2017 (Score: 0.6290)\n",
      "TSA Throughput Data August 9, 2020  to August 15, 2020 (Score: 0.6284)\n"
     ]
    }
   ],
   "source": [
    "# compute similarity scores of the sentence with the corpus\n",
    "cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "\n",
    "# Sort the results in decreasing order and get the first TOP_K\n",
    "top_results = np.argpartition(-cos_scores, range(TOP_K))[0:TOP_K]\n",
    "\n",
    "print(\"Sentence:\", sentence, \"\\n\")\n",
    "print(\"Top\", TOP_K, \"most similar sentences in corpus:\")\n",
    "for idx in top_results[0:TOP_K]:\n",
    "    print(corpus_docs.iloc[int(idx)][\"Document\"], \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
